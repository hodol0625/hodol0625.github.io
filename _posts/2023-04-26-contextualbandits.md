---
title: "contextual bandits"
comments: true
# other options

  
published: false
---

# Contextual bandits 알고리즘 소개


####  관련 Keywords
- MAB(Multi-armed bandits)
- Thompson sampling
- context
- exploration-exploitation trade-off


## 문제 상황

(언제, 어느 Biz 상황에서 써야 하는지)
MAB라는 개념 소개, 대부분의 현실 세계에서는 context라는 게 존재함.

context와 exploration - exploitation 의 trade-off 가 존재함.

그 trade-off 속에서 reward를 최대화 하는 알고리즘이 필요함.


### 어떻게 cumulative reward sum을 최대화 할 것인가?
e-greedy, thompson sampling, UCB, BootstrappedTS, Deep learning

#### e-greedy

#### thompson sampling

#### UCB

#### Bootstrapping

#### Deep neural network


---
Ref
- Li, Lihong, et al. "A contextual-bandit approach to personalized news article recommendation." Proceedings of the 19th international conference on World wide web. 2010
- Cortes, David. "Adapting multi-armed bandits policies to contextual bandits scenarios." arXiv preprint arXiv:1811.04383 (2018).
- Xu, Pan, et al. "Neural contextual bandits with deep representation and shallow exploration." arXiv preprint arXiv:2012.01780 (2020).
